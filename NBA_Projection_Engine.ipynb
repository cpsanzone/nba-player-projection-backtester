{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZhZ993fiSsZzPxGFXZlRx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cpsanzone/nba-player-projection-backtester/blob/main/NBA_Projection_Engine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5efe6b91",
        "outputId": "08c8646d-22b4-42b9-bb5d-fb671b944262"
      },
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# --- 1. Define the player_configs list ---\n",
        "player_configs = [\n",
        "    {\"name\": \"LeBron James\", \"player_files\": [\"/lebron_2023.csv\", \"/lebron_2024.csv\", \"/lebron_2025.csv\"], \"team_abbr\": \"LAL\"},\n",
        "    {\"name\": \"Nikola Jokic\", \"player_files\": [\"/jokic_2023.csv\", \"/jokic_2024.csv\", \"/jokic_2025.csv\"], \"team_abbr\": \"DEN\"},\n",
        "    {\"name\": \"Luka Doncic\", \"player_files\": [\"/luka_2023.csv\", \"/luka_2024.csv\", \"/luka_2025.csv\"], \"team_abbr\": None}, # Handles trades\n",
        "    {\"name\": \"Giannis Antetokounmpo\", \"player_files\": [\"/giannis_2023.csv\", \"/giannis_2024.csv\", \"/giannis_2025.csv\"], \"team_abbr\": \"MIL\"},\n",
        "    {\"name\": \"Shai Gilgeous-Alexander\", \"player_files\": [\"/sga_2023.csv\", \"/sga_2024.csv\", \"/sga_2025.csv\"], \"team_abbr\": \"OKC\"},\n",
        "    {\"name\": \"Austin Reaves\", \"player_files\": [\"/reaves_2023.csv\", \"/reaves_2024.csv\", \"/reaves_2025.csv\"], \"team_abbr\": \"LAL\"},\n",
        "    {\"name\": \"Tyrese Maxey\", \"player_files\": [\"/maxey_2023.csv\", \"/maxey_2024.csv\", \"/maxey_2025.csv\"], \"team_abbr\": \"PHI\"},\n",
        "    {\"name\": \"Donovan Mitchell\", \"player_files\": [\"/mitchell_2023.csv\", \"/mitchell_2024.csv\", \"/mitchell_2025.csv\"], \"team_abbr\": \"CLE\"},\n",
        "    {\"name\": \"Devin Booker\", \"player_files\": [\"/booker_2023.csv\", \"/booker_2024.csv\", \"/booker_2025.csv\"], \"team_abbr\": \"PHO\"},\n",
        "    {\"name\": \"Lauri Markkanen\", \"player_files\": [\"/lauri_2023.csv\", \"/lauri_2024.csv\", \"/lauri_2025.csv\"], \"team_abbr\": \"UTA\"},\n",
        "    {\"name\": \"Jalen Brunson\", \"player_files\": [\"/brunson_2023.csv\", \"/brunson_2024.csv\", \"/brunson_2025.csv\"], \"team_abbr\": \"NYK\"},\n",
        "    {\"name\": \"Jaylen Brown\", \"player_files\": [\"/brown_2023.csv\", \"/brown_2024.csv\", \"/brown_2025.csv\"], \"team_abbr\": \"BOS\"},\n",
        "    {\"name\": \"Cade Cunningham\", \"player_files\": [\"/cade_2023.csv\", \"/cade_2024.csv\", \"/cade_2025.csv\"], \"team_abbr\": \"DET\"}\n",
        "]\n",
        "\n",
        "# --- 2. Define the team_odds_map dictionary ---\n",
        "team_odds_map = {\n",
        "    \"LAL\": {\"files\": [\"/odds_data_2023.csv\", \"/odds_data_2024.csv\", \"/odds_data_2025.csv\"], \"seasons\": [\"2022-23\", \"2023-24\", \"2024-25\"]},\n",
        "    \"DEN\": {\"files\": [\"/nuggets_odds_2023.csv\", \"/nuggets_odds_2024.csv\", \"/nuggets_odds_2025.csv\"], \"seasons\": [\"2022-23\", \"2023-24\", \"2024-25\"]},\n",
        "    \"DAL\": {\"files\": [\"/mavs_odds_2023.csv\", \"/mavs_odds_2024.csv\", \"/mavs_odds_2025.csv\"], \"seasons\": [\"2022-23\", \"2023-24\", \"2024-25\"]},\n",
        "    \"MIL\": {\"files\": [\"/bucks_odds_2023.csv\", \"/bucks_odds_2024.csv\", \"/bucks_odds_2025.csv\"], \"seasons\": [\"2022-23\", \"2023-24\", \"2024-25\"]},\n",
        "    \"OKC\": {\"files\": [\"/thunder_odds_2023.csv\", \"/thunder_odds_2024.csv\", \"/thunder_odds_2025.csv\"], \"seasons\": [\"2022-23\", \"2023-24\", \"2024-25\"]},\n",
        "    \"PHI\": {\"files\": [\"/76ers_odds_2023.csv\", \"/76ers_odds_2024.csv\", \"/76ers_odds_2025.csv\"], \"seasons\": [\"2022-23\", \"2023-24\", \"2024-25\"]},\n",
        "    \"CLE\": {\"files\": [\"/cavs_odds_2023.csv\", \"/cavs_odds_2024.csv\", \"/cavs_odds_2025.csv\"], \"seasons\": [\"2022-23\", \"2023-24\", \"2024-25\"]},\n",
        "    \"PHO\": {\"files\": [\"/suns_odds_2023.csv\", \"/suns_odds_2024.csv\", \"/suns_odds_2025.csv\"], \"seasons\": [\"2022-23\", \"2023-24\", \"2024-25\"]},\n",
        "    \"UTA\": {\"files\": [\"/jazz_odds_2023.csv\", \"/jazz_odds_2024.csv\", \"/jazz_odds_2025.csv\"], \"seasons\": [\"2022-23\", \"2023-24\", \"2024-25\"]},\n",
        "    \"NYK\": {\"files\": [\"/knicks_odds_2023.csv\", \"/knicks_odds_2024.csv\", \"/knicks_odds_2025.csv\"], \"seasons\": [\"2022-23\", \"2023-24\", \"2024-25\"]},\n",
        "    \"BOS\": {\"files\": [\"/celtics_odds_2023.csv\", \"/celtics_odds_2024.csv\", \"/celtics_odds_2025.csv\"], \"seasons\": [\"2022-23\", \"2023-24\", \"2024-25\"]},\n",
        "    \"DET\": {\"files\": [\"/pistons_odds_2023.csv\", \"/pistons_odds_2024.csv\", \"/pistons_odds_2025.csv\"], \"seasons\": [\"2022-23\", \"2023-24\", \"2024-25\"]}\n",
        "}\n",
        "\n",
        "# Define directories\n",
        "player_gamelogs_dir = \"/player_gamelogs/\"\n",
        "odds_dir = \"/\"\n",
        "\n",
        "# --- 3. Implement create_dummy_player_file ---\n",
        "def create_dummy_player_file(player_config, directory):\n",
        "    player_name = player_config[\"name\"]\n",
        "    team_abbr_config = player_config[\"team_abbr\"]\n",
        "\n",
        "    for file_path_template in player_config[\"player_files\"]:\n",
        "        year_str = os.path.basename(file_path_template).split('_')[-1].split('.')[0]\n",
        "        try:\n",
        "            file_year = int(year_str)\n",
        "            season_start_year = file_year - 1 # Season YYYY is (YYYY-1)-YYYY\n",
        "        except ValueError:\n",
        "            print(f\"Skipping {file_path_template} due to invalid year format: {year_str}\")\n",
        "            continue\n",
        "\n",
        "        # Determine the team for this specific game log\n",
        "        if team_abbr_config is None:\n",
        "            # For Luka, randomly assign a team from the odds map teams\n",
        "            teams_for_random = [tc for tc in team_odds_map.keys() if tc is not None]\n",
        "            if not teams_for_random:\n",
        "                actual_team_for_log = \"UNK\" # Fallback\n",
        "            else:\n",
        "                actual_team_for_log = np.random.choice(teams_for_random)\n",
        "        else:\n",
        "            actual_team_for_log = team_abbr_config\n",
        "\n",
        "        data = {\n",
        "            'Date': pd.to_datetime(pd.date_range(f'{season_start_year}-10-01', periods=20, freq='D')),\n",
        "            'Tm': [actual_team_for_log] * 20,\n",
        "            'PTS': np.random.randint(10, 40, 20),\n",
        "            'AST': np.random.randint(2, 15, 20),\n",
        "            'TRB': np.random.randint(3, 18, 20),\n",
        "            'FG3M': np.random.randint(0, 7, 20),\n",
        "            'GS': np.random.randint(0, 1, 20),\n",
        "            'MP': np.random.randint(20, 40, 20)\n",
        "        }\n",
        "        df = pd.DataFrame(data)\n",
        "\n",
        "        filepath = os.path.join(directory, os.path.basename(file_path_template))\n",
        "        df.to_csv(filepath, index=False)\n",
        "\n",
        "\n",
        "# --- 4. Implement create_dummy_odds_file ---\n",
        "def create_dummy_odds_file(team_abbr, season_start_year, expected_filename, directory):\n",
        "    data = {\n",
        "        'Date': pd.to_datetime(pd.date_range(f'{season_start_year}-10-01', periods=20, freq='D')),\n",
        "        'O/U': [f\"O/U {np.random.randint(220, 240)}.0\"] * 20,\n",
        "        'ATS': [f\"ATS {np.random.randint(-10, 10)}.0\"] * 20\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "    filepath = os.path.join(directory, os.path.basename(expected_filename))\n",
        "    df.to_csv(filepath, index=False)\n",
        "\n",
        "\n",
        "# --- 5. Create directories and clear existing dummy files ---\n",
        "os.makedirs(player_gamelogs_dir, exist_ok=True)\n",
        "for f in os.listdir(player_gamelogs_dir):\n",
        "    os.remove(os.path.join(player_gamelogs_dir, f))\n",
        "print(f\"‚úÖ Cleared existing dummy player files in {player_gamelogs_dir}.\")\n",
        "\n",
        "# Clear existing dummy odds files\n",
        "# Note: This clears files from the root directory which might be sensitive.\n",
        "# For a real project, use a dedicated sub-directory for dummy odds.\n",
        "for team_abbr, config in team_odds_map.items():\n",
        "    for f in config[\"files\"]:\n",
        "        filepath = os.path.join(odds_dir, os.path.basename(f))\n",
        "        if os.path.exists(filepath):\n",
        "            os.remove(filepath)\n",
        "print(f\"‚úÖ Cleared existing dummy odds files in {odds_dir}.\")\n",
        "\n",
        "\n",
        "# --- 6. Generate all player dummy game log CSVs ---\n",
        "print(\"--- Generating Player Dummy Files ---\")\n",
        "for config in player_configs:\n",
        "    create_dummy_player_file(config, player_gamelogs_dir)\n",
        "print(\"‚úÖ All player dummy game log CSVs generated.\")\n",
        "\n",
        "\n",
        "# --- 7. Generate all team dummy odds CSVs ---\n",
        "print(\"\\n--- Generating Team Odds Dummy Files ---\")\n",
        "for team_abbr, config in team_odds_map.items():\n",
        "    for i, f in enumerate(config[\"files\"]):\n",
        "        year1 = int(config[\"seasons\"][i].split('-')[0])\n",
        "        create_dummy_odds_file(team_abbr, year1, f, odds_dir)\n",
        "print(\"‚úÖ All team dummy odds CSVs generated.\")\n",
        "\n",
        "\n",
        "# --- 8. Load, rename, and concatenate player data into df_player_master_comprehensive ---\n",
        "all_player_dfs_comprehensive = []\n",
        "print(\"\\n--- Loading All Player Stats from Dummy Files ---\")\n",
        "for config in player_configs:\n",
        "    player_name = config[\"name\"]\n",
        "    for file_path_template in config[\"player_files\"]:\n",
        "        filename = os.path.basename(file_path_template)\n",
        "        filepath = os.path.join(player_gamelogs_dir, filename)\n",
        "        if not os.path.exists(filepath):\n",
        "            print(f\"‚ö†Ô∏è Warning: Dummy player file {filepath} not found during loading. Skipping.\")\n",
        "            continue\n",
        "        try:\n",
        "            df_player = pd.read_csv(filepath)\n",
        "            df_player['Player'] = player_name # Assign the correct player name\n",
        "            df_player = df_player.rename(columns={'Tm': 'Team_Abbr'}) # Ensure 'Team_Abbr' column\n",
        "            all_player_dfs_comprehensive.append(df_player)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Warning: Could not load player file {filepath}. Error: {e}\")\n",
        "\n",
        "if all_player_dfs_comprehensive:\n",
        "    df_player_master_comprehensive = pd.concat(all_player_dfs_comprehensive, ignore_index=True)\n",
        "    print(\"‚úÖ All player dummy data loaded and concatenated into df_player_master_comprehensive.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No player game log files found or could not be loaded for df_player_master_comprehensive.\")\n",
        "    df_player_master_comprehensive = pd.DataFrame() # Ensure it's defined\n",
        "\n",
        "\n",
        "# --- 9. Clean df_player_master_comprehensive ---\n",
        "print(\"\\n--- Cleaning and Type Converting df_player_master_comprehensive ---\")\n",
        "# Ensure 'Date' column is datetime and normalize it (remove time component if any)\n",
        "df_player_master_comprehensive['Date'] = pd.to_datetime(df_player_master_comprehensive['Date'], errors='coerce').dt.normalize()\n",
        "\n",
        "# Convert essential stat columns to numeric types, coercing errors\n",
        "stat_columns_to_numeric = ['PTS', 'AST', 'TRB', 'FG3M']\n",
        "for col in stat_columns_to_numeric:\n",
        "    if col in df_player_master_comprehensive.columns:\n",
        "        df_player_master_comprehensive[col] = pd.to_numeric(df_player_master_comprehensive[col], errors='coerce')\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Warning: Column '{col}' not found in df_player_master_comprehensive. Skipping conversion.\")\n",
        "\n",
        "# Drop rows with any missing values in essential columns after conversions\n",
        "essential_columns_for_dropna = ['Date', 'PTS', 'Team_Abbr', 'AST', 'TRB', 'FG3M']\n",
        "existing_essential_columns = [col for col in essential_columns_for_dropna if col in df_player_master_comprehensive.columns]\n",
        "initial_rows_comp = len(df_player_master_comprehensive)\n",
        "df_player_master_comprehensive = df_player_master_comprehensive.dropna(subset=existing_essential_columns)\n",
        "rows_dropped_comp = initial_rows_comp - len(df_player_master_comprehensive)\n",
        "print(f\"‚úÖ df_player_master_comprehensive cleaned. Dropped {rows_dropped_comp} rows with missing essential values.\")\n",
        "print(f\"Final df_player_master_comprehensive has {len(df_player_master_comprehensive)} rows after cleaning.\")\n",
        "\n",
        "\n",
        "# --- 10. Add 'SEASON', 'PRA', and 'Performance_Score' to df_player_master_comprehensive ---\n",
        "print(\"\\n--- Engineering Features for df_player_master_comprehensive ---\")\n",
        "def get_season_str(date_obj):\n",
        "    if pd.isna(date_obj):\n",
        "        return None\n",
        "    if date_obj.month >= 10:\n",
        "        return f\"{date_obj.year}-{(date_obj.year + 1) % 100:02d}\"\n",
        "    else:\n",
        "        return f\"{date_obj.year - 1}-{(date_obj.year) % 100:02d}\"\n",
        "\n",
        "df_player_master_comprehensive['SEASON'] = df_player_master_comprehensive['Date'].apply(get_season_str)\n",
        "df_player_master_comprehensive = df_player_master_comprehensive.sort_values(by=['Player', 'Date'])\n",
        "\n",
        "df_player_master_comprehensive['PRA'] = df_player_master_comprehensive['PTS'] + df_player_master_comprehensive['TRB'] + df_player_master_comprehensive['AST']\n",
        "df_player_master_comprehensive['Performance_Score'] = df_player_master_comprehensive['PTS'] + df_player_master_comprehensive['AST'] + df_player_master_comprehensive['TRB'] + df_player_master_comprehensive['FG3M']\n",
        "\n",
        "df_player_master_comprehensive = df_player_master_comprehensive.dropna(subset=['SEASON', 'PRA', 'Performance_Score'])\n",
        "print(\"‚úÖ 'SEASON', 'PRA', and 'Performance_Score' added to df_player_master_comprehensive.\")\n",
        "print(f\"Final df_player_master_comprehensive has {len(df_player_master_comprehensive)} rows after feature engineering.\")\n",
        "\n",
        "\n",
        "# --- 11. Group by and calculate average performance scores ---\n",
        "print(\"\\n--- Calculating Average Performance Scores per Player/Team/Season ---\")\n",
        "player_season_team_avg_stats = df_player_master_comprehensive.groupby(['SEASON', 'Team_Abbr', 'Player']).agg(\n",
        "    Avg_Performance_Score=('Performance_Score', 'mean'),\n",
        "    Avg_PTS=('PTS', 'mean'),\n",
        "    Avg_AST=('AST', 'mean'),\n",
        "    Avg_TRB=('TRB', 'mean'),\n",
        "    Avg_FG3M=('FG3M', 'mean')\n",
        ").reset_index()\n",
        "print(\"‚úÖ Average performance scores computed.\")\n",
        "\n",
        "\n",
        "# --- 12. Select top 1 or 2 players per team per season ---\n",
        "print(\"\\n--- Selecting Top Players per Team per Season ---\")\n",
        "player_season_team_avg_stats_sorted = player_season_team_avg_stats.sort_values(\n",
        "    by=['SEASON', 'Team_Abbr', 'Avg_Performance_Score'],\n",
        "    ascending=[True, True, False]\n",
        ")\n",
        "top_players_per_team_season = player_season_team_avg_stats_sorted.groupby(['SEASON', 'Team_Abbr']).head(2)\n",
        "print(\"‚úÖ Top 1-2 players identified per team per season.\")\n",
        "\n",
        "\n",
        "# --- 13. Filter df_player_master_comprehensive to create df_player_master ---\n",
        "print(\"\\n--- Creating df_player_master with Selected Players ---\")\n",
        "top_player_identifiers = top_players_per_team_season[['SEASON', 'Player']].drop_duplicates()\n",
        "df_player_master = pd.merge(\n",
        "    df_player_master_comprehensive,\n",
        "    top_player_identifiers,\n",
        "    on=['SEASON', 'Player'],\n",
        "    how='inner'\n",
        ")\n",
        "print(\"‚úÖ df_player_master created, containing only games from dynamically selected top players.\")\n",
        "print(f\"Final df_player_master has {len(df_player_master)} rows.\")\n",
        "\n",
        "\n",
        "# --- 14. Load, rename, and concatenate odds data into df_odds_master ---\n",
        "all_odds_dfs = []\n",
        "print(\"\\n--- Loading All Team Odds from Dummy Files ---\")\n",
        "for team_abbr, config in team_odds_map.items():\n",
        "    for i, f in enumerate(config[\"files\"]):\n",
        "        year1 = int(config[\"seasons\"][i].split('-')[0])\n",
        "        filepath = os.path.join(odds_dir, os.path.basename(f))\n",
        "        if not os.path.exists(filepath):\n",
        "            print(f\"‚ö†Ô∏è Warning: Dummy odds file {filepath} not found during loading. Skipping.\")\n",
        "            continue\n",
        "        try:\n",
        "            df_season = pd.read_csv(filepath)\n",
        "            df_season['Team_Abbr'] = team_abbr\n",
        "            # Normalize date to remove time component for consistent merging\n",
        "            df_season['Date'] = pd.to_datetime(df_season['Date'], errors='coerce').dt.normalize()\n",
        "            all_odds_dfs.append(df_season)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Warning: Could not load odds file {filepath}. Error: {e}\")\n",
        "\n",
        "if all_odds_dfs:\n",
        "    df_odds_master = pd.concat(all_odds_dfs)\n",
        "    print(\"‚úÖ All team dummy odds loaded and concatenated into df_odds_master.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No odds dataframes loaded or could not be loaded for df_odds_master.\")\n",
        "    df_odds_master = pd.DataFrame() # Ensure it's defined\n",
        "\n",
        "\n",
        "# --- 15. Process df_odds_master into df_odds_clean ---\n",
        "print(\"\\n--- Processing df_odds_master into df_odds_clean ---\")\n",
        "df_odds_master = df_odds_master.rename(columns={'O/U': 'GAME_TOTAL_RAW', 'ATS': 'GAME_SPREAD_RAW'})\n",
        "df_odds_master['GAME_TOTAL'] = df_odds_master['GAME_TOTAL_RAW'].str.split(' ').str[-1]\n",
        "df_odds_master['GAME_TOTAL'] = pd.to_numeric(df_odds_master['GAME_TOTAL'], errors='coerce')\n",
        "df_odds_master['GAME_SPREAD'] = df_odds_master['GAME_SPREAD_RAW'].str.split(' ').str[-1]\n",
        "df_odds_master['GAME_SPREAD'] = pd.to_numeric(df_odds_master['GAME_SPREAD'], errors='coerce').abs()\n",
        "\n",
        "df_odds_clean = df_odds_master.dropna(subset=['Date', 'GAME_SPREAD', 'GAME_TOTAL', 'Team_Abbr'])\n",
        "df_odds_clean = df_odds_clean[['Date', 'GAME_SPREAD', 'GAME_TOTAL', 'Team_Abbr']].copy().drop_duplicates()\n",
        "print(\"‚úÖ df_odds_clean created and cleaned.\")\n",
        "print(f\"Final df_odds_clean has {len(df_odds_clean)} rows.\")\n",
        "\n",
        "print(\"\\n--- Block 1: Simulation Setup & Data Generation (Hardcoded Configurations) Completed --- \")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Cleared existing dummy player files in /player_gamelogs/.\n",
            "‚úÖ Cleared existing dummy odds files in /.\n",
            "--- Generating Player Dummy Files ---\n",
            "‚úÖ All player dummy game log CSVs generated.\n",
            "\n",
            "--- Generating Team Odds Dummy Files ---\n",
            "‚úÖ All team dummy odds CSVs generated.\n",
            "\n",
            "--- Loading All Player Stats from Dummy Files ---\n",
            "‚úÖ All player dummy data loaded and concatenated into df_player_master_comprehensive.\n",
            "\n",
            "--- Cleaning and Type Converting df_player_master_comprehensive ---\n",
            "‚úÖ df_player_master_comprehensive cleaned. Dropped 0 rows with missing essential values.\n",
            "Final df_player_master_comprehensive has 780 rows after cleaning.\n",
            "\n",
            "--- Engineering Features for df_player_master_comprehensive ---\n",
            "‚úÖ 'SEASON', 'PRA', and 'Performance_Score' added to df_player_master_comprehensive.\n",
            "Final df_player_master_comprehensive has 780 rows after feature engineering.\n",
            "\n",
            "--- Calculating Average Performance Scores per Player/Team/Season ---\n",
            "‚úÖ Average performance scores computed.\n",
            "\n",
            "--- Selecting Top Players per Team per Season ---\n",
            "‚úÖ Top 1-2 players identified per team per season.\n",
            "\n",
            "--- Creating df_player_master with Selected Players ---\n",
            "‚úÖ df_player_master created, containing only games from dynamically selected top players.\n",
            "Final df_player_master has 780 rows.\n",
            "\n",
            "--- Loading All Team Odds from Dummy Files ---\n",
            "‚úÖ All team dummy odds loaded and concatenated into df_odds_master.\n",
            "\n",
            "--- Processing df_odds_master into df_odds_clean ---\n",
            "‚úÖ df_odds_clean created and cleaned.\n",
            "Final df_odds_clean has 720 rows.\n",
            "\n",
            "--- Block 1: Simulation Setup & Data Generation (Hardcoded Configurations) Completed --- \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5xAodOiHCFni"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26861fc9",
        "outputId": "82ba52b6-45e3-47be-dd39-0de8638ddfa3"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1. Initialize an empty list named `all_player_results`\n",
        "all_player_results = []\n",
        "\n",
        "# 2. Define the grid search parameters (same values as original STEP 2)\n",
        "spread_values_to_test = [3, 4, 5, 6, 7, 8, 9, 10, 100]\n",
        "total_values_to_test = [\n",
        "    0, 220, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235,\n",
        "    236, 237, 238, 239, 240, 241, 242\n",
        "]\n",
        "rolling_windows_to_test = [5, 10, 15]\n",
        "pts_adjust_to_test = [-2.0, -2.5, -3.0, -3.5, -4.0, -4.5, -5.0, -5.5, -6.0]\n",
        "ast_adjust_to_test = [-0.5, -1.0, -1.5, -2.0, -2.5, -3.0]\n",
        "trb_adjust_to_test = [-0.5, -1.0, -1.5, -2.0, -2.5, -3.0]\n",
        "pra_adjust_to_test = [-2.0, -3.0, -4.0, -5.0, -6.0, -7.0]\n",
        "tpm_adjust_to_test = [-0.5, -1.0, -1.5, -2.0, -2.5, -3.0] # for FG3M\n",
        "\n",
        "print(\"--- Starting Forecasting Engine ---\")\n",
        "\n",
        "# 3. Merge df_player_master and df_odds_clean into df_merged\n",
        "# Ensure df_player_master and df_odds_clean are available from previous steps\n",
        "if 'df_player_master' not in locals() or df_player_master.empty:\n",
        "    raise ValueError(\"df_player_master is empty or not defined. Please ensure Block 1 is executed correctly.\")\n",
        "if 'df_odds_clean' not in locals() or df_odds_clean.empty:\n",
        "    raise ValueError(\"df_odds_clean is empty or not defined. Please ensure Block 1 is executed correctly.\")\n",
        "\n",
        "df_merged = pd.merge(df_player_master, df_odds_clean, on=['Date', 'Team_Abbr'], how='inner')\n",
        "print(f\"‚úÖ Master stats and odds merged. Found {len(df_merged)} total matching games.\")\n",
        "\n",
        "# 4. Define a function `get_season_str`\n",
        "def get_season_str(date_obj):\n",
        "    if pd.isna(date_obj):\n",
        "        return None\n",
        "    if date_obj.month >= 10:\n",
        "        return f\"{date_obj.year}-{(date_obj.year + 1) % 100:02d}\"\n",
        "    else:\n",
        "        return f\"{date_obj.year - 1}-{(date_obj.year) % 100:02d}\"\n",
        "\n",
        "# 5. Apply `get_season_str` to create 'SEASON' column\n",
        "df_merged['SEASON'] = df_merged['Date'].apply(get_season_str)\n",
        "\n",
        "# 6. Sort `df_merged` by 'Player' and 'Date'\n",
        "df_merged = df_merged.sort_values(by=['Player', 'Date'])\n",
        "\n",
        "# 7. Ensure statistical columns are numeric\n",
        "stat_cols_numeric_check = ['PTS', 'TRB', 'AST', 'FG3M']\n",
        "for col in stat_cols_numeric_check:\n",
        "    if col in df_merged.columns:\n",
        "        df_merged[col] = pd.to_numeric(df_merged[col], errors='coerce')\n",
        "    else:\n",
        "        df_merged[col] = 0.0 # Create with zeros if missing to prevent errors\n",
        "        print(f\"‚ö†Ô∏è Warning: Column '{col}' not found in df_merged, created with zeros.\")\n",
        "\n",
        "# 8. Calculate 'PRA'\n",
        "df_merged['PRA'] = df_merged['PTS'] + df_merged['TRB'] + df_merged['AST']\n",
        "\n",
        "# Drop any rows that might have become NaN due to coercing errors after calculations\n",
        "df_merged = df_merged.dropna(subset=['PTS', 'TRB', 'AST', 'FG3M', 'PRA', 'SEASON'])\n",
        "print(\"‚úÖ Features engineered and data cleaned for grid search.\")\n",
        "\n",
        "# 9. Iterate through each `window` in `rolling_windows_to_test`\n",
        "print(f\"\\n--- Running Full Grid Search for {len(df_merged)} games across {df_merged['Player'].nunique()} players ---\")\n",
        "\n",
        "for window in rolling_windows_to_test:\n",
        "    print(f\"Testing {window}-game window...\")\n",
        "    # a. Calculate rolling mean for 'PTS', 'AST', 'TRB', 'PRA', and 'FG3M'\n",
        "    df_merged[f'AVG_PTS'] = df_merged.groupby(['Player', 'SEASON'])['PTS'].shift(1).rolling(window, min_periods=window).mean()\n",
        "    df_merged[f'AVG_AST'] = df_merged.groupby(['Player', 'SEASON'])['AST'].shift(1).rolling(window, min_periods=window).mean()\n",
        "    df_merged[f'AVG_TRB'] = df_merged.groupby(['Player', 'SEASON'])['TRB'].shift(1).rolling(window, min_periods=window).mean()\n",
        "    df_merged[f'AVG_PRA'] = df_merged.groupby(['Player', 'SEASON'])['PRA'].shift(1).rolling(window, min_periods=window).mean()\n",
        "    df_merged[f'AVG_FG3M'] = df_merged.groupby(['Player', 'SEASON'])['FG3M'].shift(1).rolling(window, min_periods=window).mean()\n",
        "\n",
        "    # b. Create `df_testable` by dropping rows with NaN in new average columns\n",
        "    df_testable = df_merged.dropna(subset=['AVG_PTS', 'AVG_AST', 'AVG_TRB', 'AVG_PRA', 'AVG_FG3M'])\n",
        "\n",
        "    # c. Iterate through each `max_spread` and `min_total`\n",
        "    for max_spread in spread_values_to_test:\n",
        "        for min_total in total_values_to_test:\n",
        "            # i. Filter `df_testable` to create `df_filtered`\n",
        "            df_filtered = df_testable[(df_testable['GAME_SPREAD'] <= max_spread) & (df_testable['GAME_TOTAL'] >= min_total)].copy()\n",
        "\n",
        "            # ii. Calculate `total_games`\n",
        "            total_games = len(df_filtered)\n",
        "            if total_games == 0: continue\n",
        "\n",
        "            # iii. For each adjustment value, calculate `bet_line`, `wins`, and append results\n",
        "            for adj in pts_adjust_to_test:\n",
        "                bet_line = df_filtered[f'AVG_PTS'] + adj\n",
        "                wins = (df_filtered['PTS'] > bet_line).sum()\n",
        "                all_player_results.append({'stat': 'PTS', 'window': window, 'adj': adj, 'spread': max_spread, 'total': min_total, 'wins': wins, 'bets': total_games})\n",
        "\n",
        "            for adj in ast_adjust_to_test:\n",
        "                bet_line = df_filtered[f'AVG_AST'] + adj\n",
        "                wins = (df_filtered['AST'] > bet_line).sum()\n",
        "                all_player_results.append({'stat': 'AST', 'window': window, 'adj': adj, 'spread': max_spread, 'total': min_total, 'wins': wins, 'bets': total_games})\n",
        "\n",
        "            for adj in trb_adjust_to_test:\n",
        "                bet_line = df_filtered[f'AVG_TRB'] + adj\n",
        "                wins = (df_filtered['TRB'] > bet_line).sum()\n",
        "                all_player_results.append({'stat': 'TRB', 'window': window, 'adj': adj, 'spread': max_spread, 'total': min_total, 'wins': wins, 'bets': total_games})\n",
        "\n",
        "            for adj in pra_adjust_to_test:\n",
        "                bet_line = df_filtered[f'AVG_PRA'] + adj\n",
        "                wins = (df_filtered['PRA'] > bet_line).sum()\n",
        "                all_player_results.append({'stat': 'PRA', 'window': window, 'adj': adj, 'spread': max_spread, 'total': min_total, 'wins': wins, 'bets': total_games})\n",
        "\n",
        "            for adj in tpm_adjust_to_test:\n",
        "                bet_line = df_filtered[f'AVG_FG3M'] + adj\n",
        "                wins = (df_filtered['FG3M'] > bet_line).sum()\n",
        "                all_player_results.append({'stat': 'FG3M', 'window': window, 'adj': adj, 'spread': max_spread, 'total': min_total, 'wins': wins, 'bets': total_games})\n",
        "\n",
        "print(\"\\n‚úÖ‚úÖ‚úÖ All players processed. Grid search completed. ‚úÖ‚úÖ‚úÖ\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Forecasting Engine ---\n",
            "‚úÖ Master stats and odds merged. Found 780 total matching games.\n",
            "‚úÖ Features engineered and data cleaned for grid search.\n",
            "\n",
            "--- Running Full Grid Search for 780 games across 13 players ---\n",
            "Testing 5-game window...\n",
            "Testing 10-game window...\n",
            "Testing 15-game window...\n",
            "\n",
            "‚úÖ‚úÖ‚úÖ All players processed. Grid search completed. ‚úÖ‚úÖ‚úÖ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed96f8d8",
        "outputId": "0c461780-9995-43bb-c816-9fd76c174719"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"--- Aggregating Results & Identifying Best Strategies ---\")\n",
        "\n",
        "# 1. Convert the all_player_results list into a pandas DataFrame named df_results.\n",
        "# Ensure all_player_results is not empty before proceeding\n",
        "if not all_player_results:\n",
        "    print(\"No results found in all_player_results. Exiting aggregation.\")\n",
        "    # Ensure df_results is defined, even if empty, for subsequent steps if desired.\n",
        "    df_results = pd.DataFrame()\n",
        "else:\n",
        "    df_results = pd.DataFrame(all_player_results)\n",
        "\n",
        "    # 2. Group df_results by stat, window, adj, spread, and total. Aggregate wins and bets.\n",
        "    df_agg = df_results.groupby(['stat', 'window', 'adj', 'spread', 'total']).agg(\n",
        "        total_wins=('wins', 'sum'),\n",
        "        total_bets=('bets', 'sum')\n",
        "    ).reset_index()\n",
        "    print(\"‚úÖ Results aggregated by strategy parameters.\")\n",
        "\n",
        "    # 3. Calculate the win_rate for each strategy in df_agg\n",
        "    df_agg['win_rate'] = (df_agg['total_wins'] / df_agg['total_bets']) * 100\n",
        "    print(\"‚úÖ Win rates calculated.\")\n",
        "\n",
        "    # 4. Determine the 'universe size' for each stat and window combination.\n",
        "    df_universe = df_agg[\n",
        "        (df_agg['spread'] == 100) & (df_agg['total'] == 0)\n",
        "    ].groupby(['stat', 'window'])['total_bets'].max().reset_index()\n",
        "    df_universe = df_universe.rename(columns={'total_bets': 'universe_size'})\n",
        "    print(\"‚úÖ Universe sizes calculated.\")\n",
        "\n",
        "    # 5. Merge df_agg with df_universe on stat and window\n",
        "    df_agg = pd.merge(df_agg, df_universe, on=['stat', 'window'])\n",
        "    print(\"‚úÖ Opportunity universe merged.\")\n",
        "\n",
        "    # 6. Calculate the opportunity_pct for each strategy in df_agg\n",
        "    df_agg['opportunity_pct'] = (df_agg['total_bets'] / df_agg['universe_size']) * 100\n",
        "    print(\"‚úÖ Opportunity percentages calculated.\")\n",
        "\n",
        "    # 7. Define min_win_rate and min_opportunity_pct\n",
        "    min_win_rate = 75.0\n",
        "    min_opportunity_pct = 15.0\n",
        "\n",
        "    # 8. Filter df_agg to create df_agg_reliable\n",
        "    df_agg_reliable = df_agg[\n",
        "        (df_agg['win_rate'] >= min_win_rate) &\n",
        "        (df_agg['opportunity_pct'] >= min_opportunity_pct)\n",
        "    ].copy()\n",
        "    print(f\"‚úÖ Filtered for reliable strategies (Min {min_win_rate}% Win Rate, Min {min_opportunity_pct}% Opportunity Rate).\")\n",
        "\n",
        "    # 9. Sort df_agg_reliable by win_rate in descending order.\n",
        "    df_agg_reliable = df_agg_reliable.sort_values(by='win_rate', ascending=False)\n",
        "    print(\"‚úÖ Reliable strategies sorted by win rate.\")\n",
        "\n",
        "    # 10. Define a list of stats_to_compare\n",
        "    stats_to_compare = ['PTS', 'AST', 'TRB', 'PRA', 'FG3M']\n",
        "\n",
        "    print(\"\\n--- BEST RELIABLE STRATEGIES (Win Rate vs. Opportunity) ---\")\n",
        "    print(f\"(Based on {df_merged['Player'].nunique()} players, Min {min_win_rate}% Win Rate, Min {min_opportunity_pct}% Opportunity Rate)\\n\")\n",
        "\n",
        "    # 11. Loop through stats_to_compare and print best strategy details\n",
        "    for stat_type in stats_to_compare:\n",
        "        df_stat = df_agg_reliable[df_agg_reliable['stat'] == stat_type]\n",
        "\n",
        "        if df_stat.empty:\n",
        "            print(f\"No reliable strategy found for **{stat_type}**.\\n\")\n",
        "            continue\n",
        "\n",
        "        best_strategy = df_stat.iloc[0] # Get the top strategy after sorting\n",
        "\n",
        "        print(f\"üèÜ **Best for {stat_type}:**\")\n",
        "        print(f\"   Bet **{best_strategy['stat']}** using **{best_strategy['window']}-game avg {best_strategy['adj']}**\")\n",
        "        print(f\"   when: **Spread <= {best_strategy['spread']}** & **Total >= {best_strategy['total']}**\")\n",
        "        print(f\"   Win Rate: **{best_strategy['win_rate']:.2f}%** ({best_strategy['total_wins']} wins in {best_strategy['total_bets']} games)\")\n",
        "        print(f\"   (This strategy applies to **{best_strategy['opportunity_pct']:.1f}%** of all bettable games)\\n\")\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Aggregating Results & Identifying Best Strategies ---\n",
            "‚úÖ Results aggregated by strategy parameters.\n",
            "‚úÖ Win rates calculated.\n",
            "‚úÖ Universe sizes calculated.\n",
            "‚úÖ Opportunity universe merged.\n",
            "‚úÖ Opportunity percentages calculated.\n",
            "‚úÖ Filtered for reliable strategies (Min 75.0% Win Rate, Min 15.0% Opportunity Rate).\n",
            "‚úÖ Reliable strategies sorted by win rate.\n",
            "\n",
            "--- BEST RELIABLE STRATEGIES (Win Rate vs. Opportunity) ---\n",
            "(Based on 13 players, Min 75.0% Win Rate, Min 15.0% Opportunity Rate)\n",
            "\n",
            "No reliable strategy found for **PTS**.\n",
            "\n",
            "üèÜ **Best for AST:**\n",
            "   Bet **AST** using **15-game avg -3.0**\n",
            "   when: **Spread <= 4** & **Total >= 233**\n",
            "   Win Rate: **83.64%** (46 wins in 55 games)\n",
            "   (This strategy applies to **28.2%** of all bettable games)\n",
            "\n",
            "No reliable strategy found for **TRB**.\n",
            "\n",
            "üèÜ **Best for PRA:**\n",
            "   Bet **PRA** using **15-game avg -6.0**\n",
            "   when: **Spread <= 8** & **Total >= 237**\n",
            "   Win Rate: **76.67%** (23 wins in 30 games)\n",
            "   (This strategy applies to **15.4%** of all bettable games)\n",
            "\n",
            "üèÜ **Best for FG3M:**\n",
            "   Bet **FG3M** using **15-game avg -3.0**\n",
            "   when: **Spread <= 3** & **Total >= 225**\n",
            "   Win Rate: **96.36%** (53 wins in 55 games)\n",
            "   (This strategy applies to **28.2%** of all bettable games)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- BLOCK 4: PARLAY STRATEGY VALIDATOR ---\n",
        "# Verifying the mathematical edge of combining high-confidence adjusted lines.\n",
        "\n",
        "def validate_parlay_edge(leg_win_rate, num_legs):\n",
        "    \"\"\"\n",
        "    Calculates the true win probability of a parlay compared to market odds.\n",
        "    User Strategy: Combine 3-4 adjusted lines to reach ~+100 (Even Money) odds.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- PARLAY EDGE CALCULATOR ({num_legs}-Leg Strategy) ---\")\n",
        "\n",
        "    # 1. Inputs\n",
        "    market_odds_american = 100  # +100 Odds (Standard Target)\n",
        "    market_implied_prob = 50.0  # +100 implies a 50% chance of winning\n",
        "\n",
        "    # 2. Math\n",
        "    # The 'True' probability is the leg_win_rate to the power of num_legs\n",
        "    true_parlay_prob = (leg_win_rate / 100) ** num_legs * 100\n",
        "\n",
        "    # 3. Output\n",
        "    print(f\"Stats per Leg:\")\n",
        "    print(f\"  ‚Ä¢ Individual Leg Win Rate: {leg_win_rate:.2f}%\")\n",
        "    print(f\"  ‚Ä¢ Number of Legs: {num_legs}\")\n",
        "\n",
        "    print(f\"\\nMathematical Reality:\")\n",
        "    print(f\"  ‚Ä¢ Market Implied Win % (+100 Odds): {market_implied_prob:.1f}%\")\n",
        "    print(f\"  ‚Ä¢ Your Model's Win %:               {true_parlay_prob:.1f}%\")\n",
        "\n",
        "    # 4. Verdict\n",
        "    edge = true_parlay_prob - market_implied_prob\n",
        "    if edge > 0:\n",
        "        print(f\"\\n‚úÖ EDGE FOUND: +{edge:.1f}% Advantage vs The House\")\n",
        "        print(\"   Conclusion: This parlay hits significantly more often than the odds imply.\")\n",
        "    else:\n",
        "        print(f\"\\n‚ùå NO EDGE: -{abs(edge):.1f}% Disadvantage\")\n",
        "\n",
        "# --- EXECUTE VALIDATION ---\n",
        "if 'df_agg_reliable' in locals() and not df_agg_reliable.empty:\n",
        "    # Grab the best win rate from Cell 3 (e.g., your 96.36% on FG3M)\n",
        "    best_leg_win_rate = df_agg_reliable.iloc[0]['win_rate']\n",
        "\n",
        "    # Test a 3-Leg Parlay\n",
        "    validate_parlay_edge(leg_win_rate=best_leg_win_rate, num_legs=3)\n",
        "\n",
        "    # Test a 4-Leg Parlay\n",
        "    validate_parlay_edge(leg_win_rate=best_leg_win_rate, num_legs=4)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No reliable strategies found in Block 3. Using baseline.\")\n",
        "    validate_parlay_edge(leg_win_rate=85.0, num_legs=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZaQFNWj5CTUt",
        "outputId": "7c934638-20a4-4de2-bdbb-0d0b47c3ce9e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- PARLAY EDGE CALCULATOR (3-Leg Strategy) ---\n",
            "Stats per Leg:\n",
            "  ‚Ä¢ Individual Leg Win Rate: 96.36%\n",
            "  ‚Ä¢ Number of Legs: 3\n",
            "\n",
            "Mathematical Reality:\n",
            "  ‚Ä¢ Market Implied Win % (+100 Odds): 50.0%\n",
            "  ‚Ä¢ Your Model's Win %:               89.5%\n",
            "\n",
            "‚úÖ EDGE FOUND: +39.5% Advantage vs The House\n",
            "   Conclusion: This parlay hits significantly more often than the odds imply.\n",
            "\n",
            "--- PARLAY EDGE CALCULATOR (4-Leg Strategy) ---\n",
            "Stats per Leg:\n",
            "  ‚Ä¢ Individual Leg Win Rate: 96.36%\n",
            "  ‚Ä¢ Number of Legs: 4\n",
            "\n",
            "Mathematical Reality:\n",
            "  ‚Ä¢ Market Implied Win % (+100 Odds): 50.0%\n",
            "  ‚Ä¢ Your Model's Win %:               86.2%\n",
            "\n",
            "‚úÖ EDGE FOUND: +36.2% Advantage vs The House\n",
            "   Conclusion: This parlay hits significantly more often than the odds imply.\n"
          ]
        }
      ]
    }
  ]
}